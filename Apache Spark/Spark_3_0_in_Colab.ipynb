{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark 3.0 in Colab",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMh1qVdXDnj/b4d1Ksfg9kt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "brtLpwwEe-cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import local files from course\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "!unzip ml-100k.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j7Sndv0XMEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing Spark 3.0.0-preview2 with dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "print('Success1')\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "print('Success2')\n",
        "!tar xf spark-3.0.0-preview2-bin-hadoop2.7.tgz\n",
        "print('Success3')\n",
        "!pip install -q findspark\n",
        "print('Success4')\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop2.7\"\n",
        "print('Success5')\n",
        "import findspark\n",
        "findspark.init()\n",
        "print('Success6')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14BabKFmgev_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing Spark on MovieLens dataset ( how many 5 stars, 4 stars etc.)\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import collections\n",
        "\n",
        "# Spark Configuration\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Ratings\")\n",
        "# getOrCreate() = creates new SparkContext\n",
        "context = SparkContext.getOrCreate(conf = conf)\n",
        "\n",
        "# Load Dataset\n",
        "data = context.textFile(\"ml-100k/u.data\")\n",
        "# Parse data\n",
        "ratings = data.map(lambda x: x.split()[2])\n",
        "# Split data\n",
        "result = ratings.countByValue()\n",
        "\n",
        "sortResults = collections.OrderedDict(sorted(result.items()))\n",
        "for key,value in sortResults.items():\n",
        "  print(\"%s %i\" % (key,value))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}