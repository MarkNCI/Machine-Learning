{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark Basics",
      "provenance": [],
      "authorship_tag": "ABX9TyM6ADqZUIB6SGWzMoNG1Vwf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3RzyzlCoYPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Differences to Spark 2.x.x versions\n",
        "\n",
        "# Better performance (17x faster)\n",
        "# Deprecating Python 2\n",
        "# Moving from Graphx to SparkGraph (uses Cypher like Neo4j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_96xzGnHRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Introduction to Spark\n",
        "\n",
        "# Fast/general engine for large-scale data processing\n",
        "# Spark Script (Python, Scala etc) is called Driver Program\n",
        "# Has a built-in Cluster Manager, or run on Hadoop using YARN\n",
        "# Spark will split work among Executors (usually one per machine/cluster)\n",
        "# Uses DAG engine (Directed Acyclic graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PweRmtQ0qjCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## PySpark vs Scala\n",
        "\n",
        "# No need to compile, manage dependencies etc.\n",
        "# Less code overhead\n",
        "# Scala is native to Spark\n",
        "# New features then to come to Scala first\n",
        "# Speed: Scala > PySpark (slightly)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7mjECXLs7jC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Resilient Distributed Dataset (RDD)\n",
        "\n",
        "# A dataset for Big Data objects\n",
        "# Can be spread across a cluster + handle executor failures via Cluster Manager\n",
        "# SparkContext (sc in code) creates RDD and makes it resilient and distributed\n",
        "# Created by driver program\n",
        "# map, flatmap and filter are common ways to transform RDD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS0s8umAvdD2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}