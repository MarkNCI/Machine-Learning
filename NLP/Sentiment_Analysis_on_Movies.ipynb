{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis on Movies",
      "provenance": [],
      "authorship_tag": "ABX9TyP+YjVPDBldcEHqEz/5hPMO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiKCf9we6MYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Field of Text Classification\n",
        "# From some corpus, classify if positive, negative or neutral\n",
        "# Corpus typically called a document, can be large or short\n",
        "# Good for unstructured data\n",
        "\n",
        "## Bag of Words: \n",
        "# Think table with column for words + number of times it appears in document"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soD1u4PW5x-p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "62923dcf-7f10-4c7f-edbb-ed45f02279ab"
      },
      "source": [
        "## Sentiment Analyis on Tweets\n",
        "\n",
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt') # Pretrained model helps tokenise words/sentences\n",
        "nltk.download('wordnet') # Lexical English language db for finding base word\n",
        "nltk.download('averaged_perceptron_tagger')# Context of word in sentence"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ0wO0Dx6WCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenise data (tokens = strings split into smaller parts)\n",
        "# 5k positive tweets, 5k negative tweets and 20k no sentiments\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "positiveTweets = twitter_samples.strings('positive_tweets.json')\n",
        "negativeTweets = twitter_samples.strings('negative_tweets.json')\n",
        "unknownTweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
        "tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "\n",
        "# Print 1st tweet tokenised with pos_tag for word context\n",
        "#print(pos_tag(tokens[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbkMNHBn8gJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "935204c6-f92d-4568-c5be-5356be1d21d2"
      },
      "source": [
        "# Normalising Data\n",
        "# Stemming and Lemmatisation, same result but stem may not be word (mice > mouse, better > good)\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "def lemmatiseSentence(token):\n",
        "  lemmatiser = WordNetLemmatizer()\n",
        "  lemmaSentence = []\n",
        "  for word,tag in pos_tag(token):\n",
        "    if tag.startswith('NN'):\n",
        "      pos = 'n'\n",
        "    elif tag.startswith('VB'):\n",
        "      pos = 'v'\n",
        "    else:\n",
        "      pos = 'a'\n",
        "    lemmaSentence.append(lemmatiser.lemmatize(word,pos))\n",
        "  return lemmaSentence\n",
        "\n",
        "print(tokens[0])\n",
        "print(lemmatiseSentence(tokens[0]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
            "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}