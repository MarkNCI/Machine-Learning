{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOHd3nf1DPu9dYWgHOM3/C"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwmmRTdM2Ac5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d86a501-86ed-4a00-cfeb-df4ead071781"
      },
      "source": [
        "# Lowercasing\r\n",
        "sentence = \"My favourite cities in Ireland are Dublin,, Galway and Belfast\"\r\n",
        "sentence = sentence.lower()\r\n",
        "print(sentence)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my favourite cities in ireland are dublin, galway and belfast\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8apiYeij8QjA",
        "outputId": "88b8491f-49bd-4f0d-ea92-485a8d8221ff"
      },
      "source": [
        "words = ['HellO','My','NAMe','is','BOB']\r\n",
        "words = [word.lower() for word in words]\r\n",
        "print(words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello', 'my', 'name', 'is', 'bob']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMhVoHpz8ibY"
      },
      "source": [
        "# Noise removal\r\n",
        "import re\r\n",
        "def clean_words(text):\r\n",
        "  # remove html markup\r\n",
        "  text = re.sub(\"(<.*?>)\",\"\",text)\r\n",
        "  #remove non-ascii and digits\r\n",
        "  text = re.sub(\"(\\W|\\d+)\",\" \",text)\r\n",
        "  #remove whitespace\r\n",
        "  text = text.strip()\r\n",
        "  return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R14q6PdE8idT",
        "outputId": "5d2214b4-e473-4e50-8206-2d46769a8240"
      },
      "source": [
        "raw_text = ['...Hello','Hello!!','#Hello','>>>>Hello>>>','<a>Hello</a>']\r\n",
        "clean_text = [clean_words(r) for r in raw_text]\r\n",
        "print(clean_text)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'Hello', 'Hello', 'Hello', 'Hello']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "nVyqX0xL-EDZ",
        "outputId": "77b08185-3562-4c7d-f779-a2d4074f0a50"
      },
      "source": [
        "# Stemming\r\n",
        "import nltk\r\n",
        "import pandas as pd\r\n",
        "from nltk.stem import PorterStemmer as ps\r\n",
        "\r\n",
        "stemmer = ps()\r\n",
        "words = ['troubling','troubled','troubles']\r\n",
        "stems = [stemmer.stem(word=word) for word in words]\r\n",
        "\r\n",
        "df = pd.DataFrame({'Raw Word':words,'Stem':stems})\r\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Raw Word</th>\n",
              "      <th>Stem</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>troubling</td>\n",
              "      <td>troubl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>troubled</td>\n",
              "      <td>troubl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>troubles</td>\n",
              "      <td>troubl</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Raw Word    Stem\n",
              "0  troubling  troubl\n",
              "1   troubled  troubl\n",
              "2   troubles  troubl"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "uOUwmFUQB-ZC",
        "outputId": "57b390a7-ca25-478b-d3f7-2bade1517acb"
      },
      "source": [
        "# Lemmatization\r\n",
        "from nltk.stem import WordNetLemmatizer as wnl\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "lemma = wnl()\r\n",
        "lemmatized = [lemma.lemmatize(word = word, pos = 'v') for word in words]\r\n",
        "\r\n",
        "df = pd.DataFrame({'Raw Word':words,'Lemma':lemmatized})\r\n",
        "df = df[['Raw Word','Lemma']]\r\n",
        "df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Raw Word</th>\n",
              "      <th>Lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>troubling</td>\n",
              "      <td>trouble</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>troubled</td>\n",
              "      <td>trouble</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>troubles</td>\n",
              "      <td>trouble</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Raw Word    Lemma\n",
              "0  troubling  trouble\n",
              "1   troubled  trouble\n",
              "2   troubles  trouble"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nym6lmaC75J",
        "outputId": "7801ff15-852c-481b-9ea0-85146174a93a"
      },
      "source": [
        "nltk.download('punkt')\r\n",
        "from nltk import word_tokenize\r\n",
        "\r\n",
        "sentence = \"Hi! my name is John.\"\r\n",
        "sentence_re = re.sub(\"(\\W|\\d+)\",\" \",sentence)\r\n",
        "tokens = word_tokenize(sentence_re)\r\n",
        "tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi', 'my', 'name', 'is', 'John']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JItXTRFDycR",
        "outputId": "055ece6e-926c-46e8-96e0-3d436cd8fa16"
      },
      "source": [
        "# Tokensise Sentences\r\n",
        "from nltk import sent_tokenize\r\n",
        "tokens = sent_tokenize(sentence)\r\n",
        "tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi!', 'my name is John.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awy_mPESFuJg",
        "outputId": "f16449d7-a7f4-4d1f-f25a-1bac0be1272a"
      },
      "source": [
        "# Remove Stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "sentence = 'the weather today is really hot and i went for a walk'\r\n",
        "tokens = word_tokenize(sentence)\r\n",
        "tokens = [word for word in tokens if not word in stop_words]\r\n",
        "print(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['weather', 'today', 'really', 'hot', 'went', 'walk']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7C0cKLyXPHt",
        "outputId": "7a8f3101-7e46-4e88-ef70-5ab7c9d5b9d9"
      },
      "source": [
        "# Word Embedding with Word2Vec\r\n",
        "# Word2Vec is shallow NN (2 layers)\r\n",
        "from gensim.models import Word2Vec as wtv\r\n",
        "\r\n",
        "s1 = 'Bob Marley was a Regae singer'\r\n",
        "s2 = 'He has been dead for several years'\r\n",
        "s3 = 'Bob was a great artist'\r\n",
        "\r\n",
        "# Tokenise words\r\n",
        "sentences = [word_tokenize(s1),word_tokenize(s2),word_tokenize(s3)]\r\n",
        "# Train the model (min_count = number of matching words returned)\r\n",
        "model = wtv(sentences,min_count=1)\r\n",
        "# Summarise model (vocab: Number of words submitted)\r\n",
        "print('Model Summary:')\r\n",
        "print(model)\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# Words used in models\r\n",
        "words = list(model.wv.vocab)\r\n",
        "print('Corpus Vocabulary')\r\n",
        "print(words)\r\n",
        "print('\\n')\r\n",
        "print('Vector for word singer')\r\n",
        "print(model['singer'])\r\n",
        "print('\\n')\r\n",
        "\r\n",
        "# Test on 6 words similar to corpus\r\n",
        "test = ['singer']\r\n",
        "model.wv.most_similar(positive=test,topn=6)\r\n",
        "\r\n",
        "### Loading and Saving ###\r\n",
        "# model.save('whatever')\r\n",
        "# model.load('whatever')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Summary:\n",
            "Word2Vec(vocab=15, size=100, alpha=0.025)\n",
            "\n",
            "\n",
            "Corpus Vocabulary\n",
            "['Bob', 'Marley', 'was', 'a', 'Regae', 'singer', 'He', 'has', 'been', 'dead', 'for', 'several', 'years', 'great', 'artist']\n",
            "\n",
            "\n",
            "Vector for word singer\n",
            "[-5.1021599e-04  1.3014454e-03  1.6370032e-03 -2.9160082e-03\n",
            "  1.4067261e-03  6.6956511e-04 -3.0644675e-04 -1.3621356e-03\n",
            " -2.5179225e-03  2.1225144e-03 -3.6723423e-03  4.5849211e-03\n",
            " -5.4021366e-05  4.7829011e-03  1.0308600e-03  1.7682988e-03\n",
            " -1.6815581e-03 -8.0540677e-04  4.8965057e-03  3.9657457e-03\n",
            "  1.2069247e-03  8.8147691e-04 -4.4387332e-03  4.6534979e-04\n",
            " -8.4981561e-04  1.1791082e-03 -3.9849677e-03 -3.1178207e-03\n",
            "  2.9399076e-03  2.3299740e-03  2.8398195e-05  3.2586372e-03\n",
            " -2.0716120e-03 -4.6158214e-03  1.3616442e-03  2.1065008e-03\n",
            " -2.5901163e-03  1.3368434e-03  3.1266096e-03  2.8227719e-03\n",
            "  2.3954795e-03  4.2638988e-03 -4.1238433e-03  3.3572069e-03\n",
            " -2.8746082e-03 -4.8822318e-03  2.5473782e-03  1.2847480e-03\n",
            "  8.7296830e-05 -3.1431606e-03 -1.5215476e-03  3.1854396e-03\n",
            " -2.6263315e-03  1.7729236e-03 -2.4839407e-03  1.1081472e-03\n",
            "  3.1293798e-03 -2.1280467e-03 -6.9590181e-04  3.4377824e-03\n",
            "  2.4381585e-03 -1.9284557e-03  4.5249076e-03 -3.7218067e-03\n",
            " -4.3790461e-03 -3.1963065e-03 -2.7642502e-03  3.3830283e-03\n",
            " -4.2680837e-03  2.1549049e-03  4.6011624e-03 -4.0498800e-03\n",
            " -2.1123674e-03 -6.4418575e-04 -3.2890728e-04 -2.3644303e-03\n",
            "  2.7813497e-03 -4.8162667e-03  4.0583932e-03 -2.8415609e-03\n",
            "  5.0750674e-05 -3.7634799e-03  3.5999450e-03 -3.6713197e-03\n",
            "  4.5696823e-03  4.7032251e-03 -1.5471948e-03 -2.6440050e-03\n",
            " -4.1395179e-03  1.5069925e-03  6.3275470e-04  2.3530629e-03\n",
            " -3.4409135e-03 -9.1230957e-04  3.2523985e-03 -6.6199515e-04\n",
            "  2.2000126e-03  4.2288261e-03  2.8482648e-03 -3.3864703e-03]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Bob', 0.17550605535507202),\n",
              " ('Marley', 0.09529964625835419),\n",
              " ('was', 0.07691202312707901),\n",
              " ('Regae', 0.05354451388120651),\n",
              " ('He', 0.043564509600400925),\n",
              " ('great', 0.038243331015110016)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHAtGSgsh7xk"
      },
      "source": [
        "# Word Embedding with GloVe "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnE9TUY3kuF1"
      },
      "source": [
        "# Word Embedding Example with Word2Vec...\r\n",
        "# Text8 file took long time to load"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}
