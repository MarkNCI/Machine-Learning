{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Engineering.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMwdAzk+hOPu7XUc2V8SHV+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UchEfofLzVMd"
      },
      "source": [
        "## Feature Engineering\n",
        "# Apply what you know about dataset create better features to train models\n",
        "# Features: Age, Height, Weight if predicting heart disease\n",
        "\n",
        "## The Curse of Dimensionlity\n",
        "# Too many features = Sparse Data (1 feature = 1 Dimension)\n",
        "# Select features most relevant (Age + Weight vs Height)\n",
        "# Techniques: PCA, K-Means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZsdwCIV3VHp"
      },
      "source": [
        "## Imputation Techniques for Missing Data\n",
        "# There are several techniques:\n",
        "\n",
        "# Mean replacment: Replace blank cell with mean of it's column\n",
        "# Fast + Easy: won't affect mean or sample size\n",
        "# Median may be better with outliers\n",
        "# Not the best: Not accurate, doesn't work on categorical columns (1-5), only works on columns (miss correlation between features)\n",
        "\n",
        "# Can drop rows if not many, but not the best approach either.\n",
        "# Better to sub in a similar field\n",
        "\n",
        "## Use Machine Learning:\n",
        "# KNN: Find K nearest (most similar) rows and average values (Best for numerical)\n",
        "# Deep Learning to impute data: Complicated but works well for categorical\n",
        "# Regression: Linear/Non-Linear relationships between missing values and other features\n",
        "# E.g. MICE (Multiple Imputation by Chained Equations)\n",
        "# Or else, get more data!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkApHOLl5oZF"
      },
      "source": [
        "## Unbalanced data\n",
        "# Large discrepancy between pos and neg results\n",
        "\n",
        "# Oversampling: Duplicate samples from minority classifications\n",
        "# Undersampling: Remove neg scores (Usuall not right unless big data scaling issue)\n",
        "\n",
        "## Better = SMOTE: Synthethic Minority Oversampling Technique\n",
        "# Generate new sample of minority class using nearest neighbours\n",
        "#   - KNN on each minority sample + make new sample from result (mean of neighbours)\n",
        "# Generates new samples + undersamples majority class\n",
        "\n",
        "# Can also adjust thresholds: Red = 60, Amber = 30 etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOsd6G__69GD"
      },
      "source": [
        "## Binning\n",
        "# Bucket observations together based on range of values, e.g. 20 yr olds, 30 etc.\n",
        "# Quantile binning: Categorise data by their place in distribution\n",
        "# Numeric data = ordinal data\n",
        "# Used if measurements have errors\n",
        "\n",
        "## Transforming\n",
        "# Apply function to feature to make it better for training\n",
        "# Useful if data is not linear\n",
        "\n",
        "## Encoding\n",
        "# Transform data as required by model (like Deep Learning)\n",
        "# 1-Hot Encoding: \n",
        "#   - Buckets per category\n",
        "#   - Our category is 1, other buckets are 0\n",
        "\n",
        "## Scaling/Normalisation\n",
        "# Some models prefer feature data to be normally distributed around 0/scaled similarly\n",
        "\n",
        "# Can also shuffle data\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}